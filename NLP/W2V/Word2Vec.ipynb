{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jerrykuo7727/word2vec#%E3%80%88Gensim-Word2Vec-%E7%B0%A1%E6%98%93%E6%95%99%E5%AD%B8%E3%80%89\n",
    "#http://zake7749.github.io/2016/08/28/word2vec-with-gensim/\n",
    "#https://medium.com/pyladies-taiwan/%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86%E5%85%A5%E9%96%80-word2vec%E5%B0%8F%E5%AF%A6%E4%BD%9C-f8832d9677c8\n",
    "'''\n",
    "Word2Vec:非監督學習，理論上資料及越大越好\n",
    "Word2vec 依靠了 skip-gram 與 Continuous Bag of Word (CBOW) 的方法來實作，核心是一個極為淺層的類神經網路。\n",
    "透過使每個字詞與前後字詞的向量相近，來訓練出含有每個字詞語義的字詞向量\n",
    "Word2vec 對於文字處理的領域帶來了巨大的改變，\n",
    "後面也可以再使用 Convolution Neural Network 來做進階的辨識等方法。\n",
    "\n",
    "但如果將一份文件中每個字詞的向量加總，試圖來獲得這份文章的 vector 時，\n",
    "依舊是會發生失去了順序性的問題，下一篇我將介紹 Doc2vec 這方法。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#輸入資料集\n",
    "def process_dirs(base_dir):\n",
    "    df = pd.DataFrame(columns=[\"cat\", \"text\"])\n",
    "    \n",
    "    for dir_path, dir_name, file_names in os.walk(base_dir):\n",
    "        for single_file in file_names:\n",
    "                try:\n",
    "                    f = open(os.path.join(dir_path, single_file), \"r\", encoding=\"utf-8\")\n",
    "                    content = f.read()\n",
    "                    content = content.replace(\"\\r\",'').replace(\"\\n\", '')\n",
    "                    split_word = jieba.lcut(content)\n",
    "                    s = pd.Series([dir_path.split(\"\\\\\")[1],split_word], index=[\"cat\",\"text\"])\n",
    "                    df = df.append(s, ignore_index = True)\n",
    "                except UnicodeDecodeError:\n",
    "                    pass\n",
    "    df['cat'] = df['cat'].astype('category')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\keneau\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 3.446 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "base_dir= 'chinese_news_trans'\n",
    "test_dir = 'chinese_news_test'\n",
    "train_df = process_dirs(base_dir)\n",
    "test_df = process_dirs(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2738"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.concat([train_df.text, test_df.text]).sample(frac=1)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用sample，frac=1的時候對於行做shuffle\n",
    "corpus = pd.concat([train_df.text, test_df.text]).sample(frac=1)\n",
    "corpus_test = corpus.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(w2v_model, words, topn=10):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "伊拉克 not found in Word2Vec model!\n",
      "導演 not found in Word2Vec model!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>和平</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>們</td>\n",
       "      <td>0.998802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>幹</td>\n",
       "      <td>0.998707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>專家</td>\n",
       "      <td>0.998703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>同</td>\n",
       "      <td>0.998604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>聯合</td>\n",
       "      <td>0.998564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>最高</td>\n",
       "      <td>0.998486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>第四</td>\n",
       "      <td>0.998384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>行業</td>\n",
       "      <td>0.998332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>組建</td>\n",
       "      <td>0.998324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>愛</td>\n",
       "      <td>0.998245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   和平       cos\n",
       "0   們  0.998802\n",
       "1   幹  0.998707\n",
       "2  專家  0.998703\n",
       "3   同  0.998604\n",
       "4  聯合  0.998564\n",
       "5  最高  0.998486\n",
       "6  第四  0.998384\n",
       "7  行業  0.998332\n",
       "8  組建  0.998324\n",
       "9   愛  0.998245"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(model, ['伊拉克', '和平', '導演'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec 參數調整指南\n",
    "#ref:https://radimrehurek.com/gensim/models/word2vec.html\n",
    "'''\n",
    "準備一個訓練快速的陽春模型，先把feature處理到你滿意為止再開始調整模型 \n",
    "feature決定了準確度的上限，調整模型只是讓我們盡可能接近這個上限而已 \n",
    "Kaggler投入在資料探索與feature engineering的時間，往往會是調整模型的兩倍以上 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size=100：詞向量的維度大小，維度太小會無法有效表達詞與詞的關係，維度太大會使關係太稀疏而難以找出規則 \n",
    "#Kaggle比賽上常用的詞向量維度介於200到300之間\n",
    "#iter=5：訓練的回數，訓練過少會使得詞關係過為鬆散，訓練過度又會使得詞關係過為極端\n",
    "#當使用較大的詞向量維度時，可能會需要訓練更多次，我們先用iter=10來看看結果\n",
    "#min_count=5：出現次數大於等於min_count的詞，才會納入Word2Vec的詞典中\n",
    "#window=5：CBOW下決定Word2Vec一次取多少詞來預測中間詞\n",
    "#max_vocab_size=None：Word2Vec的詞典容納上限，出現次數最低的詞會優先被剔除\n",
    "#hs=0：hs=0時採用Negative Sampling，hs=1時採用Hierarchical Softmax\n",
    "#workers=3：訓練用的線程數量（可以加快訓練速度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus,iter=5,size=200,window=5,min_count=5,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>伊拉克</th>\n",
       "      <th>cos</th>\n",
       "      <th>和平</th>\n",
       "      <th>cos</th>\n",
       "      <th>導演</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>此次</td>\n",
       "      <td>0.818974</td>\n",
       "      <td>中東</td>\n",
       "      <td>0.946329</td>\n",
       "      <td>西洋</td>\n",
       "      <td>0.950848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>戰爭</td>\n",
       "      <td>0.807849</td>\n",
       "      <td>兩國</td>\n",
       "      <td>0.907003</td>\n",
       "      <td>散文</td>\n",
       "      <td>0.944279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>海灣</td>\n",
       "      <td>0.796979</td>\n",
       "      <td>外交</td>\n",
       "      <td>0.876159</td>\n",
       "      <td>京劇</td>\n",
       "      <td>0.940952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>伊朗</td>\n",
       "      <td>0.774241</td>\n",
       "      <td>友誼</td>\n",
       "      <td>0.875554</td>\n",
       "      <td>古典</td>\n",
       "      <td>0.940103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>演習</td>\n",
       "      <td>0.762786</td>\n",
       "      <td>各國</td>\n",
       "      <td>0.868877</td>\n",
       "      <td>曲藝</td>\n",
       "      <td>0.938228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>士兵</td>\n",
       "      <td>0.760707</td>\n",
       "      <td>歐洲</td>\n",
       "      <td>0.859499</td>\n",
       "      <td>神韻</td>\n",
       "      <td>0.937542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>布什</td>\n",
       "      <td>0.760571</td>\n",
       "      <td>民主</td>\n",
       "      <td>0.854557</td>\n",
       "      <td>唱法</td>\n",
       "      <td>0.937123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>越南</td>\n",
       "      <td>0.748433</td>\n",
       "      <td>阿拉伯</td>\n",
       "      <td>0.853337</td>\n",
       "      <td>幼兒</td>\n",
       "      <td>0.935728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>敘利亞</td>\n",
       "      <td>0.736598</td>\n",
       "      <td>柬埔寨</td>\n",
       "      <td>0.852852</td>\n",
       "      <td>詩歌</td>\n",
       "      <td>0.934469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>美軍事</td>\n",
       "      <td>0.728926</td>\n",
       "      <td>共同</td>\n",
       "      <td>0.852595</td>\n",
       "      <td>佳作</td>\n",
       "      <td>0.933675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   伊拉克       cos   和平       cos  導演       cos\n",
       "0   此次  0.818974   中東  0.946329  西洋  0.950848\n",
       "1   戰爭  0.807849   兩國  0.907003  散文  0.944279\n",
       "2   海灣  0.796979   外交  0.876159  京劇  0.940952\n",
       "3   伊朗  0.774241   友誼  0.875554  古典  0.940103\n",
       "4   演習  0.762786   各國  0.868877  曲藝  0.938228\n",
       "5   士兵  0.760707   歐洲  0.859499  神韻  0.937542\n",
       "6   布什  0.760571   民主  0.854557  唱法  0.937123\n",
       "7   越南  0.748433  阿拉伯  0.853337  幼兒  0.935728\n",
       "8  敘利亞  0.736598  柬埔寨  0.852852  詩歌  0.934469\n",
       "9  美軍事  0.728926   共同  0.852595  佳作  0.933675"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(model, ['伊拉克', '和平', '導演'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2vref:https://medium.com/life-of-small-data-engineer/%E8%83%BD%E8%A2%AB%E9%9B%BB%E8%85%A6%E7%90%86%E8%A7%A3%E7%9A%84%E6%96%87%E5%AD%97-nlp-%E4%B8%80-word-embedding-4146267019cb\n",
    "#Bag of Words (BoW):是一種將句子拆解成每個詞出現次數的方法但是卻忽略掉了很重要的次序關係\n",
    "#Word Embedding:概念是建立字詞向量，將句子中每個字轉換為向量，最後結合起來變成矩陣。\n",
    "'''\n",
    "Word Embedding可以保留句子中字詞的順序，且改變為適合電腦運算的矩陣形式，但產生了新的問題稀疏矩陣「當字詞過多時每個字的向量會太長，轉換的矩陣亦會更加龐大，且這矩陣中有大量的欄位皆為 0」。\n",
    "依照 stanfordnlp/GloVe 預先訓練好的 word vector 中，可以用 300 維的向量來表示兩百二十萬個字詞，可以有效解決上述的維度爆炸問題，節省了大量的運算及儲存成本。\n",
    "這麼一來使得電腦多了許多額外的運算及儲存成本，而且整體的效率十分低，後來有了更好的方法(Word2vec)。\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
